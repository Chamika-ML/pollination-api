{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3e41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import folium\n",
    "from folium.raster_layers import ImageOverlay\n",
    "from folium import plugins\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime,timezone,timedelta\n",
    "import pytz #time zone data\n",
    "\n",
    "import exifread\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#commented all the print functions because the api not working in aws deployment when we have print functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccabf0",
   "metadata": {},
   "source": [
    "## Spatial Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e057d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffrence_between_tow_points(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    \"\"\"This funtion finds the distance between two locations in Km when the longitudes and latitudes of the two points are given\"\"\"\n",
    "    \n",
    "    R = 6371 # radius of the eatch in kilo meters \n",
    "    lon1_rad = math.radians(lon1) # convert degrees to radians\n",
    "    lon2_rad = math.radians(lon2)    \n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lat2_rad = math.radians(lat2)   \n",
    "    \n",
    "    del_lon = lon2_rad - lon1_rad\n",
    "    del_lat = lat2_rad - lat1_rad\n",
    "    \n",
    "    a = (math.sin(del_lat/2))**2 + math.cos(lat1_rad)*math.cos(lat2_rad)*((math.sin(del_lon/2))**2)\n",
    "    c = 2*math.atan2(math.sqrt(a),math.sqrt(1-a))\n",
    "    d = R*c\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def distence_probability(dist):\n",
    "    \n",
    "    \"\"\"Takes one distencs and convert it to probability\"\"\"\n",
    "    \n",
    "    # Distence probability function\n",
    "    d1 = (np.exp(dist/100))/15.6\n",
    "    d2 = 1.63/d1 \n",
    "    prob = np.where(dist<275,d1,np.where(dist<=325,1,np.where(dist<=500,d2,0)))\n",
    "         \n",
    "    return prob\n",
    "\n",
    "\n",
    "def distance_matrix(latitudes,longitudes,hive_location_dataset):\n",
    "    \n",
    "    \"\"\"this function finds distence between each grid point and hive location\"\"\" \n",
    "    \n",
    "    grid_point_latitudes = latitudes\n",
    "    grid_point_longitudes = longitudes\n",
    "\n",
    "    hive_point_latitudes = np.array(hive_location_dataset['Latitude'])\n",
    "    hive_point_longitudes = np.array(hive_location_dataset['Longitude']) \n",
    "    \n",
    "    distance_arr = []\n",
    "\n",
    "    for i in range(len(grid_point_latitudes)):\n",
    "\n",
    "        point1 = (grid_point_latitudes[i],grid_point_longitudes[i])\n",
    "\n",
    "        distance_vec = []\n",
    "\n",
    "        for j in range(len(hive_point_latitudes)):\n",
    "\n",
    "            point2 = (hive_point_latitudes[j],hive_point_longitudes[j])\n",
    "\n",
    "            distance = geodesic(point1, point2).meters\n",
    "            distance_vec.append(distance)\n",
    "\n",
    "        distance_arr.append(distance_vec)\n",
    "        \n",
    "    return np.array(distance_arr)\n",
    "\n",
    "\n",
    "def probability_matrix(distance_matrix,hive_location_dataset):\n",
    "    \n",
    "    \"\"\"This function convert the distences to probabilities\"\"\"\n",
    "    \n",
    "    total_frames = np.array(hive_location_dataset['Total Frames'])\n",
    "    \n",
    "    prob_dist = [] # this is 2d vector containing all the probabilities of points form hives (3960*260)\n",
    "\n",
    "    for i in range(distance_matrix.shape[0]):\n",
    "\n",
    "        prob_dist_vec = [] # probabilities containing each grid point in a row (len 260)\n",
    "\n",
    "        for j in range(distance_matrix.shape[1]):\n",
    "\n",
    "            point_dist = distance_matrix[i][j] # distance from hive\n",
    "            prob = distence_probability(point_dist)\n",
    "\n",
    "            # append the (probability*total frames) crosponding distance range\n",
    "            prob_dist_vec.append(prob*total_frames[j])\n",
    "\n",
    "\n",
    "        prob_dist.append(prob_dist_vec) \n",
    "\n",
    "    prob_dist_arr = np.array(prob_dist)\n",
    "    \n",
    "    return prob_dist_arr\n",
    "\n",
    "\n",
    "def convert_one_probability(prob_dist_arr):\n",
    "    \n",
    "    \"\"\"This function takes matrics of probabilities and gives the sum of each raw them\"\"\"\n",
    "    # get the sum of each rows in the probability metrix\n",
    "    sum_prob_vec = [] # get the sum of all raws \n",
    "\n",
    "    for i in range(prob_dist_arr.shape[0]):\n",
    "\n",
    "        distance_row = prob_dist_arr[i]\n",
    "\n",
    "        sum_distance_row = np.sum(distance_row)\n",
    "\n",
    "        sum_prob_vec.append(sum_distance_row)\n",
    "\n",
    "    sum_prob_arr = np.array(sum_prob_vec) \n",
    "\n",
    "    norm_sum_prob_arr = (sum_prob_arr - np.min(sum_prob_arr))/(np.max(sum_prob_arr)-np.min(sum_prob_arr)) #  normalized sum_prob_arr using min max formula\n",
    "\n",
    "    return norm_sum_prob_arr\n",
    "\n",
    "\n",
    "def spatial_probability_dataset(lat,long):\n",
    "    \n",
    "    \"\"\"This is the finla function. this function call all the above funcions to make the sptial probabilities\"\"\"\n",
    "    \n",
    "    hive_location_dataset = pd.read_csv(\"./data/csv/hive_locations.csv\")\n",
    "    \n",
    "    distances = distance_matrix(lat,long,hive_location_dataset)\n",
    "    porbabilities = probability_matrix(distances,hive_location_dataset)  \n",
    "    norm_sum_prob_arr = convert_one_probability(porbabilities)\n",
    "    \n",
    "    data = {'Id':np.arange(1,len(norm_sum_prob_arr)+1,1), 'Longitude':long,'Latitude':lat, 'Spatial Prob':norm_sum_prob_arr}\n",
    "    dataset = pd.DataFrame(data)\n",
    "    dataset.head()\n",
    "    dataset.to_csv(\"./results/csv/Grid PDF PI.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f35b8",
   "metadata": {},
   "source": [
    "## Weather Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d8ceb5",
   "metadata": {},
   "source": [
    "#### Weather Probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "676f68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tempreture_probability(tempreture):\n",
    "\n",
    "    t1= 0.141*np.exp(tempreture/10) - 0.1\n",
    "    t2 = (3.4/t1)-0.42\n",
    "    prob = np.where(tempreture<0,0,np.where(tempreture<=20,t1,np.where(tempreture<=30,1,np.where(tempreture<=40,t2,0))))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def humidity_probability(humidity):\n",
    "    \n",
    "    h1 = 0.0322*np.exp(humidity/10)\n",
    "    h2 = 2.7/h1 \n",
    "    prob = np.where(humidity<35,h1,np.where(humidity<=45,1,h2))\n",
    "    \n",
    "    return prob\n",
    "\n",
    "def wind_probability(speed):\n",
    "                         \n",
    "    w1 = 3*np.exp(-speed/10) - 0.15\n",
    "    prob = np.where(speed<=10,1,np.where(speed>=30,0,w1))\n",
    "    return prob\n",
    "\n",
    "def hour_probability(data_set):\n",
    "    \n",
    "    \"\"\"This function returns 1 if the sun in sky, otherwise gives 0\"\"\"\n",
    "    \n",
    "    hour_prob = []\n",
    "    \n",
    "    for i in range(data_set.shape[0]):\n",
    "        \n",
    "        #get times as strings\n",
    "        sunrise_str = str(data_set[\"Sunrise\"][i]).split()[1]\n",
    "        sunset_str = str(data_set[\"Sunset\"][i]).split()[1]\n",
    "        curr_time_str = str(data_set[\"Time\"][i])[:8]\n",
    "        \n",
    "        #get time strings as time objects\n",
    "        sunrise = datetime.strptime(sunrise_str, '%H:%M:%S').time()\n",
    "        sunset = datetime.strptime(sunset_str, '%H:%M:%S').time()\n",
    "        curr_time = datetime.strptime(curr_time_str, '%H:%M:%S').time()\n",
    "        \n",
    "        #checks the current time and sunset and sunrise\n",
    "        if(sunrise<=curr_time<=sunset):\n",
    "            hour_prob.append(1.0)\n",
    "        else:\n",
    "            hour_prob.append(0.0)\n",
    "            \n",
    "    return hour_prob\n",
    "\n",
    "                         \n",
    "def final_probability(data_set,lat,long):\n",
    "    \n",
    "    #load spatial porbability data set\n",
    "    try:\n",
    "        spatial_prob_data = pd.read_csv(\"./results/csv/Grid PDF PI.csv\")\n",
    "    except:\n",
    "        #print(\"No Spatial data, need to create spatial probability dataset\")\n",
    "        #if the spatial_prob_data not there need to careate it and then call it.\n",
    "        spatial_probability_dataset(lat,long)\n",
    "        spatial_prob_data = pd.read_csv(\"./results/csv/Grid PDF PI.csv\")        \n",
    "    \n",
    "    #load weather description porbability data set\n",
    "    weather_desc_data = pd.read_excel(\"./data/csv/weather_description_map.xlsx\")\n",
    "    # genarate weather probability using mean ratings\n",
    "    weather_desc_data[\"Probability\"] = (weather_desc_data[\"Mean Ratings\"]-1)/(10-1)\n",
    "\n",
    "    data_set[\"Weather condition prob\"] = list(weather_desc_data[weather_desc_data[\"Weather ID\"]==list(data_set[\"Weather ID\"])[0]][\"Probability\"])[0]\n",
    "\n",
    "    \n",
    "    #hour probability\n",
    "    hour_prob_arr = hour_probability(data_set)\n",
    "    data_set[\"hour prob\"] = hour_prob_arr\n",
    "    \n",
    "    data_set[\"tempreture prob\"] = data_set[\"Tempreture\"].apply(tempreture_probability)\n",
    "    data_set[\"humidity prob\"] = data_set[\"Humidity\"].apply(humidity_probability)\n",
    "    data_set[\"wind prob\"] = data_set[\"Wind speed\"].apply(wind_probability)\n",
    "                                    \n",
    "                                    \n",
    "    prob = np.array(data_set[\"tempreture prob\"]*data_set[\"humidity prob\"]*data_set[\"wind prob\"]* data_set[\"Weather condition prob\"]*data_set[\"hour prob\"])\n",
    "    data_set[\"Weather Prob\"] = prob\n",
    "    \n",
    "    final_data_set = pd.merge(data_set,spatial_prob_data, on='Id')\n",
    "    final_data_set[\"Final Prob\"] = final_data_set[\"Weather Prob\"]*final_data_set[\"Spatial Prob\"]\n",
    "    final_data_set.drop(columns=[\"Longitude_y\",\"Latitude_y\"], axis=1, inplace = True)\n",
    "    final_data_set.rename(columns = {'Longitude_x':'Longitude', 'Latitude_x':'Latitude'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    return final_data_set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe52eaa",
   "metadata": {},
   "source": [
    "#### Weather data donwload functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fadcb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_weather_data_raw(latitudes,longitudes,cols,speed_up=4):\n",
    "    \n",
    "    # this function extract the weather data from api when provide the lat and long arrays (each raw of latitude)\n",
    "    # the speed_up factor  determines that how may weather data values paeted by previous copied value, here it is pasted 3 values (4-1=3) by previous copied value.\n",
    "    # here cols means number of points in a raw\n",
    "    # create a data frame\n",
    "    grid_point_Weather_data = pd.DataFrame(columns=[\"Time\",\"Longitude\", \"Latitude\",\"Tempreture\", \"Humidity\",\"Wind speed\",\"Weather ID\", \"Weather ID group\", \"Weather ID description\", \"Sunrise\", \"Sunset\"])\n",
    "    srt_time  = datetime.now()\n",
    "    piangil_timezone = pytz.timezone('Australia/Sydney')\n",
    "\n",
    "    for i in range(int(cols/speed_up)): # contralls the amount of the data\n",
    "\n",
    "        srt_time_point  = datetime.now()\n",
    "        #get the lat long coordinates\n",
    "        lat = latitudes[speed_up*i] \n",
    "        long = longitudes[speed_up*i]\n",
    "\n",
    "        #API url\n",
    "        url = \"https://api.openweathermap.org/data/2.5/weather?lat={}&lon={}&appid=8ee842d65cf08ec205365865e3d53348&units=metric\".format(lat,long)\n",
    "\n",
    "\n",
    "        piangil_time = datetime.now(piangil_timezone) #get time in Australia for data set\n",
    "\n",
    "        #get data form API as json data\n",
    "        res = requests.get(url)\n",
    "        data = res.json()\n",
    "\n",
    "        # create the data list that we want from the json data \n",
    "        data_vec = [piangil_time,long, lat, data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_1 = [piangil_time,longitudes[speed_up*i+1], latitudes[speed_up*i+1], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_2 = [piangil_time,longitudes[speed_up*i+2], latitudes[speed_up*i+2], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "        data_vec_3 = [piangil_time,longitudes[speed_up*i+3], latitudes[speed_up*i+3], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "\n",
    "\n",
    "        #update the data frame\n",
    "        grid_point_Weather_data.loc[speed_up*i] = data_vec\n",
    "        grid_point_Weather_data.loc[speed_up*i+1] = data_vec_1\n",
    "        grid_point_Weather_data.loc[speed_up*i+2] = data_vec_2\n",
    "        grid_point_Weather_data.loc[speed_up*i+3] = data_vec_3\n",
    "\n",
    "        # if the longitudes arr length (or raw length of the map points) can not divide by speed_up then remaining point in the columns should be filled previous values\n",
    "        if(i%((int(cols/speed_up))-1)==0) and (cols%speed_up !=0) and (i!=0):\n",
    "            num = cols%speed_up\n",
    "            for j in range(num):\n",
    "                data_vec_j = [piangil_time,longitudes[speed_up*i+3+(j+1)], latitudes[speed_up*i+3+(j+1)], data[\"main\"][\"temp\"], data[\"main\"][\"humidity\"], data[\"wind\"][\"speed\"], data[\"weather\"][0][\"id\"], data[\"weather\"][0][\"main\"], data[\"weather\"][0][\"description\"], unix_to_aus(data[\"sys\"][\"sunrise\"]), unix_to_aus(data[\"sys\"][\"sunset\"])]\n",
    "                grid_point_Weather_data.loc[speed_up*i+3+(j+1)] = data_vec_j\n",
    "                #print(f\"this is done when i is equals to {i}\")\n",
    "\n",
    "\n",
    "        time.sleep(0.1)\n",
    "        end_time_point  = datetime.now()\n",
    "        #print(f\"step {i+1} is completed! and taken {end_time_point-srt_time_point} time to complete\")\n",
    "\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    total_execution_time = end_time-srt_time\n",
    "    #print(f\"the programe take: {total_execution_time} to complete\")\n",
    "    \n",
    "          \n",
    "    return grid_point_Weather_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def download_weather_data(latitudes,longitudes,cols,raws):\n",
    "    \n",
    "    grid_point_Weather_data = pd.DataFrame(columns=[\"Time\",\"Longitude\", \"Latitude\",\"Tempreture\", \"Humidity\",\"Wind speed\",\"Weather ID\", \"Weather ID group\", \"Weather ID description\", \"Sunrise\", \"Sunset\"])\n",
    "    \n",
    "    for i in range(raws):\n",
    "        \n",
    "        # selecting each raw of latitude and longitude arrays\n",
    "        lat_arr = latitudes[i*cols:(i+1)*cols]\n",
    "        long_arr = longitudes[i*cols:(i+1)*cols] \n",
    "        \n",
    "        # get weather data for each raw of latitudes and longitudes\n",
    "        first_batch_data = download_weather_data_raw(lat_arr,long_arr,cols)\n",
    "        \n",
    "        # combine the pandas dataframe with previoues one\n",
    "        grid_point_Weather_data = pd.concat([grid_point_Weather_data,first_batch_data], axis=0, ignore_index=True)\n",
    "        #print(f\"complete the {i+1} raw data download\")\n",
    "        #print(\"==================\")\n",
    "        #print(\"==================\")\n",
    "    \n",
    "    # set the Id column and charge the raw order\n",
    "    grid_point_Weather_data[\"Id\"] = [j+1 for j in range(cols*raws)]\n",
    "    grid_point_Weather_data = grid_point_Weather_data[[\"Id\",\"Time\",\"Longitude\", \"Latitude\",\"Tempreture\", \"Humidity\",\"Wind speed\",\"Weather ID\", \"Weather ID group\", \"Weather ID description\", \"Sunrise\", \"Sunset\"]]\n",
    "    \n",
    "    return grid_point_Weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b7084",
   "metadata": {},
   "source": [
    "#### Weather data preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d888845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_aus(time):\n",
    "    \n",
    "    \"\"\"this function convert UNIX date time to Austrelia date time and output will be string. This function is called\n",
    "    inside the download_weather_data_raw function \"\"\"\n",
    "    \n",
    "    time_int = int(time) #get integer value\n",
    "    \n",
    "    time_zone = timezone(timedelta(seconds=36000)) # time zone of Austrelia \n",
    "    \n",
    "    aus_time = datetime.fromtimestamp(time_int, tz = time_zone).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    #aus_time = datetime.fromtimestamp(time_int, tz = time_zone)\n",
    "    \n",
    "    return aus_time\n",
    "\n",
    "\n",
    "\n",
    "def find_solar_seconds(Sunrise,Sunset):\n",
    "    \n",
    "    \"\"\" this function got two datetime vectors and return the time diffrence between each two elements in seconds as an array.\n",
    "    This function is called inside the solar_activation_time function\"\"\"\n",
    "    \n",
    "    Timedelta_list = [] # tiem diffrence list\n",
    "    \n",
    "    for i in range(len(Sunrise)):\n",
    "        \n",
    "        Timedelta_str_whole = Sunset[i] - Sunrise[i] # output example: Timedelta('0 days 12:17:55')\n",
    "        Timedelta_str_time = str(Timedelta_str_whole).split()[2] # output example : '12:17:55'\n",
    "        Timedelta_str_num = Timedelta_str_time.split(':') # output example: ['12', '17', '55'] \n",
    "        Timedelta_num = [int(i) for i in Timedelta_str_num] # output example: [12, 17, 55]\n",
    "        \n",
    "        Timedelta_sconds = Timedelta_num[0]*3600 + Timedelta_num[1]*60 + Timedelta_num[2] # output example: 44275\n",
    "        \n",
    "        Timedelta_list.append(Timedelta_sconds)\n",
    "        \n",
    "    return np.array(Timedelta_list)\n",
    "\n",
    "\n",
    "\n",
    "def solar_activation_time(dataset):\n",
    "    \n",
    "    \"\"\"This functions calculate the solar activation time for a day in scond using sunrice and sunset times in given\n",
    "    paddas table and add a column for it and return the dataframe\"\"\"\n",
    "    \n",
    "    # convert string type timedate data in to dateime type data\n",
    "    dataset['Sunrise'] = pd.to_datetime(dataset['Sunrise'], format='%Y-%m-%d %H:%M:%S')\n",
    "    dataset['Sunset'] = pd.to_datetime(dataset['Sunset'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    Sunrise = dataset[\"Sunrise\"]\n",
    "    Sunset = dataset[\"Sunset\"]\n",
    "\n",
    "    # find the solar activation time\n",
    "    time_diffrence_arr = find_solar_seconds(Sunrise,Sunset)\n",
    "    \n",
    "    dataset[\"Solar activation sconds\"] = time_diffrence_arr\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def add_date_time(dataset):\n",
    "    \n",
    "    \"\"\"This function add a date column and time column for a given pandas dataframe using Sunrice column data\n",
    "     and Time column data.\"\"\"\n",
    "    \n",
    "    # create a date column as first column\n",
    "    date_column = dataset[\"Sunrise\"].apply(lambda x: ((str(x)).split())[0])\n",
    "    dataset.insert(1, \"Date\",date_column)\n",
    "\n",
    "    # update the Time column\n",
    "    dataset[\"Time\"] = dataset[\"Time\"].apply(lambda x: (str(x)).split()[1][:11])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e04d78",
   "metadata": {},
   "source": [
    "## Image taken locations plot functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa6efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_metadata_exif(image_path):\n",
    "    \"\"\"his function returns the meta data ofimages\"\"\"\n",
    "    try:\n",
    "        with open(image_path, 'rb') as img_file:\n",
    "            # Get Exif tags\n",
    "            tags = exifread.process_file(img_file)\n",
    "            return tags\n",
    "    except Exception as e:\n",
    "        #print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_lat_long(list_lat,list_long):\n",
    "    \"\"\"This function convert lat long meta deta to actual lat long coordinates\"\"\"\n",
    "    #decimal degrees = degrees + minutes / 60 + seconds / 3600\n",
    "    if list_lat == \"\" or list_long == \"\":\n",
    "        lat  = np.NaN\n",
    "        long = np.NaN\n",
    "    \n",
    "    else:\n",
    "        lat = -(list_lat[0] + list_lat[1]/60 + list_lat[2]/3600)\n",
    "        long = list_long[0] + list_long[1]/60 + list_long[2]/3600 \n",
    "    \n",
    "    return lat,long\n",
    "\n",
    "def image_taken_location_dataset(img_folder_path):\n",
    "    \"\"\"This function loop througth each image and extract meta data and find lat long and frame counts and make pandas table\"\"\"\n",
    "    \n",
    "    image_names = os.listdir(img_folder_path)\n",
    "    image_psths = [f\"./images/images/{img}\" for img in image_names]\n",
    "\n",
    "    frame_count_arr = []\n",
    "    lat_arr = []\n",
    "    long_arr = []\n",
    "\n",
    "    frame_key = \"Image ImageDescription\" \n",
    "    lat_key = \"GPS GPSLatitude\"\n",
    "    long_key = \"GPS GPSLongitude\"\n",
    "\n",
    "    for img_pth in image_psths:\n",
    "\n",
    "        result = read_image_metadata_exif(img_pth)\n",
    "\n",
    "        if frame_key in result:\n",
    "            frame_count = str(result[\"Image ImageDescription\"])\n",
    "        else:\n",
    "            frame_count = np.NaN\n",
    "\n",
    "        if lat_key in result:\n",
    "            lat_list = eval(str(result[\"GPS GPSLatitude\"]))\n",
    "        else:\n",
    "            lat_list = \"\"\n",
    "\n",
    "        if long_key in result:\n",
    "            long_list = eval(str(result[\"GPS GPSLongitude\"]))\n",
    "        else:\n",
    "            long_list = \"\"\n",
    "\n",
    "        lat,long = convert_lat_long(lat_list,long_list)\n",
    "        frame_count_arr.append(frame_count)\n",
    "        lat_arr.append(lat)\n",
    "        long_arr.append(long)\n",
    "\n",
    "    data = {\"lat\":lat_arr, \"long\":long_arr, \"frame count\":frame_count_arr}\n",
    "    dataset = pd.DataFrame(data)\n",
    "    updated_dataset = dataset.dropna()\n",
    "    return updated_dataset\n",
    "\n",
    "\n",
    "def location_grid_frame_count(img_folder_path):\n",
    "    \"\"\"This function create lat long coordinate pairs for each location and preprocess the frame count of the pandas data frame and returns\"\"\"\n",
    "    \n",
    "    updated_dataset = image_taken_location_dataset(img_folder_path)\n",
    "    location = list(zip(updated_dataset[\"lat\"],updated_dataset[\"long\"]))\n",
    "    frame_count = updated_dataset[\"frame count\"].values\n",
    "    frame_count[608] = '4/4,10/10' #this point's original value contains error for the map\n",
    "    \n",
    "    return location,frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1dc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a5ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af135e26",
   "metadata": {},
   "source": [
    "## User Input Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c4df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_to_latlong():\n",
    "    \n",
    "    \"\"\"This funtion takes the user data  in form of latitudes and longitudes like this: min latitude,max latitude, min longitude, max longitude\n",
    "    and return the point grid varctors of given latitude and longitude boundries\"\"\"\n",
    "\n",
    "    user_input = input(\"Enter the Lat Long codinates separated by a comma:\")\n",
    "\n",
    "    # get and evaluate the user inputs\n",
    "    try:\n",
    "        splited_input = user_input.split(\",\")\n",
    "\n",
    "        # user can only enter four numbers\n",
    "        if(len(splited_input)==4):\n",
    "            start_latitude = float(splited_input[0])\n",
    "            end_latitude = float(splited_input[1])\n",
    "\n",
    "            start_longitude = float(splited_input[2])\n",
    "            end_longitude = float(splited_input[3])\n",
    "\n",
    "            #print(f\"Your start and end latitudes are:{[start_latitude,end_latitude]} and start and end longitudes are:{[start_longitude,end_longitude]}\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Exceed or less number of inputes. Check the inputs again.\")\n",
    "\n",
    "\n",
    "    except (ValueError,IndexError):\n",
    "        pass\n",
    "        #print(\"Error! Invalid input. Please enter valied input\")\n",
    "\n",
    "\n",
    "    # extract the data from user inputs    \n",
    "    start_lat = start_latitude\n",
    "    end_lat = end_latitude\n",
    "    start_long = start_longitude\n",
    "    end_long = end_longitude\n",
    "\n",
    "\n",
    "    separation_meters = 70\n",
    "    factor = 0.001 # for get points same as Qgis\n",
    "    separation_degrees = separation_meters/111000  #One degree of latitude is approximately 111 kilometers\n",
    "    num_of_points_lat = round(((abs(end_lat - start_lat)/factor) + 1))\n",
    "    num_of_points_long = round(((abs(end_long - start_long)/factor) + 1))\n",
    "\n",
    "\n",
    "\n",
    "    latitudes_arr = np.linspace(start_lat,end_lat,num_of_points_lat)\n",
    "    longitudes_arr = np.linspace(start_long,end_long,num_of_points_long)\n",
    "\n",
    "    # create grid points \n",
    "    point_grid = [(lat,long) for lat in latitudes_arr for long in longitudes_arr]\n",
    "    latitudes = np.array([point_grid[i][0] for i in range(len(point_grid))])\n",
    "    longitudes = np.array([point_grid[i][1] for i in range(len(point_grid))])\n",
    "    \n",
    "    # here longitudes_arr array containing the number of points in x direction (columns)\n",
    "    # here latitudes_arr array containing the number of points in ydirection (raws)\n",
    "    \n",
    "    return latitudes,longitudes,len(longitudes_arr),len(latitudes_arr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def api_to_latlong(start_lat,end_lat,start_long,end_long):\n",
    "    \"\"\"Thsi function takes min max lat longs form the api and returns the point grid varctors of given latitude and longitude boundries\"\"\"\n",
    "    \n",
    "    separation_meters = 70\n",
    "    factor = 0.001 # for get points same as Qgis\n",
    "    separation_degrees = separation_meters/111000  #One degree of latitude is approximately 111 kilometers\n",
    "    num_of_points_lat = round(((abs(end_lat - start_lat)/factor) + 1))\n",
    "    num_of_points_long = round(((abs(end_long - start_long)/factor) + 1))\n",
    "\n",
    "\n",
    "    latitudes_arr = np.linspace(start_lat,end_lat,num_of_points_lat)\n",
    "    longitudes_arr = np.linspace(start_long,end_long,num_of_points_long)\n",
    "\n",
    "    # create grid points \n",
    "    point_grid = [(lat,long) for lat in latitudes_arr for long in longitudes_arr]\n",
    "    latitudes = np.array([point_grid[i][0] for i in range(len(point_grid))])\n",
    "    longitudes = np.array([point_grid[i][1] for i in range(len(point_grid))])\n",
    "    \n",
    "    # here longitudes_arr array containing the number of points in x direction (columns)\n",
    "    # here latitudes_arr array containing the number of points in ydirection (raws)\n",
    "    \n",
    "    return latitudes,longitudes,len(longitudes_arr),len(latitudes_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5be123e",
   "metadata": {},
   "source": [
    "## User Output Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597a958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_processing_time(cols,raws,speed_up=4,download_rate=60):\n",
    "    \"\"\"thid functin will returns how much time will take for plot the heatmap\"\"\"\n",
    "    no_of_points = cols*raws\n",
    "    threshold = 2 # this is just a random value, if we need to add more time to actual time. we can increase this value \n",
    "    time_to_porcess   = int((int(no_of_points/speed_up))/download_rate) + threshold\n",
    "    return time_to_porcess\n",
    "\n",
    "def temporal_heatmap(dataset, image_path=\" \"):\n",
    "    \"\"\"This function returns the temporal heat map\"\"\"  \n",
    "    longitudes = dataset[\"Longitude\"]\n",
    "    latitudes = dataset[\"Latitude\"]\n",
    "    probability = dataset[\"Weather Prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "    \n",
    "    \"\"\"# print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        #print(\"Please provide a valid image path\")\n",
    "\n",
    "    #m.save(\"experiment.html\")\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def final_heatmap_with_image_locations(dataset, image_path=\" \"):\n",
    "    \"\"\"This function returns the final heat map with image taken locations with markers\"\"\"\n",
    "    location, frame_count = location_grid_frame_count(\"./images/images/\")\n",
    "    \n",
    "    longitudes = dataset[\"Longitude\"]\n",
    "    latitudes = dataset[\"Latitude\"]\n",
    "    probability = dataset[\"Final Prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "\n",
    "    # Add markers for each coordinate in the list\n",
    "    for i, coord in enumerate(location):\n",
    "        folium.Marker(location=coord, popup=f'{frame_count[i]}').add_to(m)\n",
    "        \n",
    "    \"\"\"# print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        #print(\"Please provide a valid image path\")\n",
    "\n",
    "    #m.save(\"experiment.html\")\n",
    "\n",
    "    return m\n",
    "\n",
    "def final_heatmap(dataset, image_path=\" \"):\n",
    "    \n",
    "    \"\"\"This function returns the final heat map\"\"\"\n",
    "    longitudes = dataset[\"Longitude\"]\n",
    "    latitudes = dataset[\"Latitude\"]\n",
    "    probability = dataset[\"Final Prob\"]\n",
    "\n",
    "    # heat map data ([lat,long,prob])\n",
    "    heatdata = [list(i) for i in list(zip(latitudes,longitudes,probability))]\n",
    "\n",
    "    # Create a base map with satellite tiles\n",
    "    m = folium.Map(location=[sum(latitudes)/len(latitudes), sum(longitudes)/len(longitudes)], \n",
    "                   zoom_start=14,\n",
    "                   tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "                   attr='Google Satellite',\n",
    "                   width='100%',\n",
    "                   height = '100%')\n",
    "    \n",
    "    # plot heatmap on the map\n",
    "    plugins.HeatMap(heatdata, radius=20, blur=20, min_opacity=0.0).add_to(m)\n",
    "     \n",
    "    \"\"\"# print each point in the map\n",
    "    for point in point_grid:\n",
    "        folium.Marker(location=point,popup=str(point)).add_to(m)\"\"\"\n",
    "    \n",
    "    # add farm image on the mapp\n",
    "    try:\n",
    "        if(image_path != \" \"):\n",
    "\n",
    "            overlay =  ImageOverlay(\n",
    "                image_path,\n",
    "                bounds= [[min(latitudes),min(longitudes)],[max(latitudes),max(longitudes)]],\n",
    "                opacity = 0.5\n",
    "            )\n",
    "\n",
    "            overlay.add_to(m)\n",
    "    except:\n",
    "        pass\n",
    "        #print(\"Please provide a valid image path\")\n",
    "\n",
    "    #m.save(\"experiment.html\")\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def final_temporal_map_api(min_lat,max_lat,min_lon,max_lon):\n",
    "    \n",
    "    \"\"\"This is the final fuction that we need to call when we are using api\"\"\"\n",
    "    \n",
    "    lat, long, cols, raws = api_to_latlong(min_lat,max_lat,min_lon,max_lon)\n",
    "    dataset = download_weather_data(lat,long,cols, raws)\n",
    "    dataset = solar_activation_time(dataset)\n",
    "    dataset = add_date_time(dataset)\n",
    "    dataset = final_probability(dataset,lat,long)\n",
    "    final_map = final_heatmap(dataset,\"./images/study_area.JPG\")\n",
    "    dataset.to_csv(\"./results/csv/final_automated_weather_data.csv\", index=False)\n",
    "    final_map.save(\"./results/maps/final_map_API_call.html\")\n",
    "    \n",
    "    \n",
    "    return final_map, dataset\n",
    "\n",
    "\n",
    "def final_temporal_map():\n",
    "    \n",
    "    \"\"\"This is the final fuction that we need to call when we are using this notebook\"\"\"\n",
    "    \n",
    "    lat, long,cols,raws  = user_input_to_latlong()\n",
    "    time_to_process = calculate_processing_time(cols, raws)\n",
    "    #print(f\"this will take {time_to_process} minutes to complete\")\n",
    "    dataset = download_weather_data(lat,long,cols,raws)\n",
    "\n",
    "    dataset = solar_activation_time(dataset)\n",
    "    dataset = add_date_time(dataset)\n",
    "    dataset = final_probability(dataset,lat,long)\n",
    "    final_map = final_heatmap(dataset,\"./images/study_area.JPG\")\n",
    "    dataset.to_csv(\"./results/csv/final_automated_weather_data3.csv\", index=False)\n",
    "    final_map.save(\"./results/maps/final_map3.html\")\n",
    "    \n",
    "    return final_map,dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e69539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897ba9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fe039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a015e25",
   "metadata": {},
   "source": [
    "### UI part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67efd216",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = [-35.083200762,-35.142200762,143.251973043,143.316973043]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14992a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Lat Long codinates separated by a comma:-35.083200762,-35.142200762,143.251973043,143.316973043\n",
      "Your start and end latitudes are:[-35.083200762, -35.142200762] and start and end longitudes are:[143.251973043, 143.316973043]\n",
      "this will take 18 minutes to complete\n",
      "step 1 is completed! and taken 0:00:01.240979 time to complete\n",
      "step 2 is completed! and taken 0:00:01.196519 time to complete\n",
      "step 3 is completed! and taken 0:00:01.089005 time to complete\n",
      "step 4 is completed! and taken 0:00:01.002211 time to complete\n",
      "step 5 is completed! and taken 0:00:01.145529 time to complete\n",
      "step 6 is completed! and taken 0:00:01.056855 time to complete\n",
      "step 7 is completed! and taken 0:00:01.041225 time to complete\n",
      "step 8 is completed! and taken 0:00:01.002427 time to complete\n",
      "step 9 is completed! and taken 0:00:00.995068 time to complete\n",
      "step 10 is completed! and taken 0:00:00.898838 time to complete\n",
      "step 11 is completed! and taken 0:00:01.414666 time to complete\n",
      "step 12 is completed! and taken 0:00:00.950008 time to complete\n",
      "step 13 is completed! and taken 0:00:01.116262 time to complete\n",
      "step 14 is completed! and taken 0:00:00.981331 time to complete\n",
      "step 15 is completed! and taken 0:00:01.039343 time to complete\n",
      "this is done when i is equals to 15\n",
      "this is done when i is equals to 15\n",
      "step 16 is completed! and taken 0:00:01.428384 time to complete\n",
      "the programe take: 0:00:17.904030 to complete\n",
      "complete the 1 raw data download\n",
      "==================\n",
      "==================\n",
      "step 1 is completed! and taken 0:00:01.244533 time to complete\n",
      "step 2 is completed! and taken 0:00:01.077104 time to complete\n",
      "step 3 is completed! and taken 0:00:01.038733 time to complete\n",
      "step 4 is completed! and taken 0:00:01.063390 time to complete\n",
      "step 5 is completed! and taken 0:00:01.168549 time to complete\n",
      "step 6 is completed! and taken 0:00:01.106555 time to complete\n",
      "step 7 is completed! and taken 0:00:00.960557 time to complete\n",
      "step 8 is completed! and taken 0:00:00.989899 time to complete\n",
      "step 9 is completed! and taken 0:00:00.888518 time to complete\n",
      "step 10 is completed! and taken 0:00:01.134366 time to complete\n",
      "step 11 is completed! and taken 0:00:01.077009 time to complete\n",
      "step 12 is completed! and taken 0:00:01.159485 time to complete\n",
      "step 13 is completed! and taken 0:00:01.088636 time to complete\n",
      "step 14 is completed! and taken 0:00:00.973981 time to complete\n",
      "step 15 is completed! and taken 0:00:01.267862 time to complete\n",
      "this is done when i is equals to 15\n",
      "this is done when i is equals to 15\n",
      "step 16 is completed! and taken 0:00:01.118582 time to complete\n",
      "the programe take: 0:00:17.357759 to complete\n",
      "complete the 2 raw data download\n",
      "==================\n",
      "==================\n",
      "step 1 is completed! and taken 0:00:01.632558 time to complete\n",
      "step 2 is completed! and taken 0:00:01.853122 time to complete\n",
      "step 3 is completed! and taken 0:00:01.213324 time to complete\n",
      "step 4 is completed! and taken 0:00:01.133986 time to complete\n",
      "step 5 is completed! and taken 0:00:01.087038 time to complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_map, data_set \u001b[38;5;241m=\u001b[39m final_temporal_map()\n\u001b[0;32m      2\u001b[0m final_map\n",
      "Cell \u001b[1;32mIn[8], line 169\u001b[0m, in \u001b[0;36mfinal_temporal_map\u001b[1;34m()\u001b[0m\n\u001b[0;32m    167\u001b[0m time_to_process \u001b[38;5;241m=\u001b[39m calculate_processing_time(cols, raws)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis will take \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_to_process\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes to complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 169\u001b[0m dataset \u001b[38;5;241m=\u001b[39m download_weather_data(lat,long,cols,raws)\n\u001b[0;32m    171\u001b[0m dataset \u001b[38;5;241m=\u001b[39m solar_activation_time(dataset)\n\u001b[0;32m    172\u001b[0m dataset \u001b[38;5;241m=\u001b[39m add_date_time(dataset)\n",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m, in \u001b[0;36mdownload_weather_data\u001b[1;34m(latitudes, longitudes, cols, raws)\u001b[0m\n\u001b[0;32m     73\u001b[0m long_arr \u001b[38;5;241m=\u001b[39m longitudes[i\u001b[38;5;241m*\u001b[39mcols:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mcols] \n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# get weather data for each raw of latitudes and longitudes\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m first_batch_data \u001b[38;5;241m=\u001b[39m download_weather_data_raw(lat_arr,long_arr,cols)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# combine the pandas dataframe with previoues one\u001b[39;00m\n\u001b[0;32m     79\u001b[0m grid_point_Weather_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([grid_point_Weather_data,first_batch_data], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 25\u001b[0m, in \u001b[0;36mdownload_weather_data_raw\u001b[1;34m(latitudes, longitudes, cols, speed_up)\u001b[0m\n\u001b[0;32m     22\u001b[0m piangil_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow(piangil_timezone) \u001b[38;5;66;03m#get time in Australia for data set\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#get data form API as json data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m res \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# create the data list that we want from the json data \u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1308\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1309\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1310\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenv\\Lib\\ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_map, data_set = final_temporal_map()\n",
    "final_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a663e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e4dba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82791c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e20e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7faa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b86bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "API key for the python programm:\n",
    "http://127.0.0.1:7777/pollination/?minLat=-35.083200762&maxLat=-35.142200762&minLon=143.251973043&maxLon=143.316973043\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
